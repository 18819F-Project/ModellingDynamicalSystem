{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZgd3UMJoRdr",
        "outputId": "0f454a1d-77bd-43b7-861b-0028f0108aa3"
      },
      "id": "OZgd3UMJoRdr",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit\n",
            "  Downloading qiskit-0.33.0.tar.gz (13 kB)\n",
            "Collecting qiskit-terra==0.19.0\n",
            "  Downloading qiskit_terra-0.19.0-cp37-cp37m-manylinux2010_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 4.7 MB/s \n",
            "\u001b[?25hCollecting qiskit-aer==0.9.1\n",
            "  Downloading qiskit_aer-0.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (17.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.9 MB 621 kB/s \n",
            "\u001b[?25hCollecting qiskit-ibmq-provider==0.18.1\n",
            "  Downloading qiskit_ibmq_provider-0.18.1-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 61.4 MB/s \n",
            "\u001b[?25hCollecting qiskit-ignis==0.7.0\n",
            "  Downloading qiskit_ignis-0.7.0-py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 59.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from qiskit-aer==0.9.1->qiskit) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from qiskit-aer==0.9.1->qiskit) (1.19.5)\n",
            "Collecting requests-ntlm>=1.1.0\n",
            "  Downloading requests_ntlm-1.1.0-py2.py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from qiskit-ibmq-provider==0.18.1->qiskit) (1.24.3)\n",
            "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.7/dist-packages (from qiskit-ibmq-provider==0.18.1->qiskit) (2.23.0)\n",
            "Collecting websocket-client>=1.0.1\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from qiskit-ibmq-provider==0.18.1->qiskit) (2.8.2)\n",
            "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/lib/python3.7/dist-packages (from qiskit-ignis==0.7.0->qiskit) (57.4.0)\n",
            "Collecting retworkx>=0.8.0\n",
            "  Downloading retworkx-0.10.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 34.7 MB/s \n",
            "\u001b[?25hCollecting symengine>=0.8\n",
            "  Downloading symengine-0.8.1-cp37-cp37m-manylinux2010_x86_64.whl (38.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.2 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting stevedore>=3.0.0\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.7/dist-packages (from qiskit-terra==0.19.0->qiskit) (1.7.1)\n",
            "Collecting python-constraint>=1.4\n",
            "  Downloading python-constraint-1.4.0.tar.bz2 (18 kB)\n",
            "Collecting scipy>=1.0\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting tweedledum<2.0,>=1.1\n",
            "  Downloading tweedledum-1.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (943 kB)\n",
            "\u001b[K     |████████████████████████████████| 943 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/dist-packages (from qiskit-terra==0.19.0->qiskit) (5.4.8)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.7/dist-packages (from qiskit-terra==0.19.0->qiskit) (0.3.4)\n",
            "Collecting ply>=3.10\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.8.0->qiskit-ibmq-provider==0.18.1->qiskit) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19->qiskit-ibmq-provider==0.18.1->qiskit) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19->qiskit-ibmq-provider==0.18.1->qiskit) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19->qiskit-ibmq-provider==0.18.1->qiskit) (2021.10.8)\n",
            "Collecting cryptography>=1.3\n",
            "  Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 19.7 MB/s \n",
            "\u001b[?25hCollecting ntlm-auth>=1.0.2\n",
            "  Downloading ntlm_auth-1.5.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.18.1->qiskit) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.18.1->qiskit) (2.21)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from stevedore>=3.0.0->qiskit-terra==0.19.0->qiskit) (4.8.2)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.8.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.7.0->stevedore>=3.0.0->qiskit-terra==0.19.0->qiskit) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.7.0->stevedore>=3.0.0->qiskit-terra==0.19.0->qiskit) (3.6.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy>=1.3->qiskit-terra==0.19.0->qiskit) (1.2.1)\n",
            "Building wheels for collected packages: qiskit, python-constraint\n",
            "  Building wheel for qiskit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qiskit: filename=qiskit-0.33.0-py3-none-any.whl size=11763 sha256=f90de4a758faa8aa63936577ad98bb51caac39aab1bd172ede9b614391fd1766\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/60/ef/c49ae113df02818c4ade5fb6e2e89bb928625412eda0bb25f5\n",
            "  Building wheel for python-constraint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-constraint: filename=python_constraint-1.4.0-py2.py3-none-any.whl size=24081 sha256=652bafd2fd27167bd2a4e6743482be7f82ceac223e922cd6fd72e9aa45c1437c\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/27/db/1222c80eb1e431f3d2199c12569cb1cac60f562a451fe30479\n",
            "Successfully built qiskit python-constraint\n",
            "Installing collected packages: pbr, tweedledum, symengine, stevedore, scipy, retworkx, python-constraint, ply, ntlm-auth, cryptography, websocket-client, requests-ntlm, qiskit-terra, qiskit-ignis, qiskit-ibmq-provider, qiskit-aer, qiskit\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed cryptography-36.0.0 ntlm-auth-1.5.0 pbr-5.8.0 ply-3.11 python-constraint-1.4.0 qiskit-0.33.0 qiskit-aer-0.9.1 qiskit-ibmq-provider-0.18.1 qiskit-ignis-0.7.0 qiskit-terra-0.19.0 requests-ntlm-1.1.0 retworkx-0.10.2 scipy-1.7.3 stevedore-3.5.0 symengine-0.8.1 tweedledum-1.1.1 websocket-client-1.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2bdf8ecf-e158-4f63-8813-9c55be438073",
      "metadata": {
        "id": "2bdf8ecf-e158-4f63-8813-9c55be438073"
      },
      "outputs": [],
      "source": [
        "import qiskit\n",
        "from qiskit.circuit.random import random_circuit\n",
        "from qiskit import transpile, assemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c763adc9-7fe5-44f6-8a9a-72f30e641562",
      "metadata": {
        "id": "c763adc9-7fe5-44f6-8a9a-72f30e641562"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch.autograd import Function\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "06531ccd-1b9f-4f08-8c98-d0cc9315f3f1",
      "metadata": {
        "tags": [],
        "id": "06531ccd-1b9f-4f08-8c98-d0cc9315f3f1"
      },
      "outputs": [],
      "source": [
        "#Custom library\n",
        "'''\n",
        "sys.path.append(\"../LQDNN\")\n",
        "sys.path.append(\"../LQDNN/model\")\n",
        "sys.path.append(\"../LQDNN/dynamics\")\n",
        "'''\n",
        "sys.path.append(\"./sample_data\")\n",
        "from visualize import *\n",
        "from ode_solver import *\n",
        "from network import *\n",
        "from dataloader import *\n",
        "from lagrangian import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3e5037ab-2b3d-45c8-94c5-cc6423ba1522",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e5037ab-2b3d-45c8-94c5-cc6423ba1522",
        "outputId": "7fd8a0dc-a144-4aeb-93de-7e034987f454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training and test data...\n",
            "Loading training and test data completed\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading training and test data...\")\n",
        "train_data = np.load('/content/sample_data/train_dataset.npz')\n",
        "train_inputs = train_data[\"input\"]\n",
        "train_labels = train_data[\"labels\"]\n",
        "\n",
        "#train_dataset = DynamicsDataset(train_inputs, train_labels)\n",
        "X_train = train_inputs[:1000]\n",
        "y_train = train_labels[:1000]\n",
        "\n",
        "#train_dataset = DynamicsDataset(train_inputs, train_labels)\n",
        "train_dataset = DynamicsDataset(X_train, y_train)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                 batch_size=1,\n",
        "                                                 shuffle=True,\n",
        "                                                 collate_fn=DynamicsDataset.collate_fn,\n",
        "                                                 pin_memory=True,\n",
        "                                                 num_workers=1)\n",
        "test_data = np.load('/content/sample_data/val_dataset.npz')\n",
        "test_inputs = train_data[\"input\"]\n",
        "test_labels = train_data[\"labels\"]\n",
        "#print(len(test_inputs))\n",
        "X_test = test_inputs[:1000]\n",
        "y_test = test_labels[:1000]\n",
        "\n",
        "test_dataset = DynamicsDataset(X_test, y_test)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                                 batch_size=1,\n",
        "                                                 shuffle=False,\n",
        "                                                 collate_fn=DynamicsDataset.collate_fn,\n",
        "                                                 pin_memory=True,\n",
        "                                                 num_workers=1)\n",
        "print(\"Loading training and test data completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3243769e-9364-42ff-826e-e93dd4c23994",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3243769e-9364-42ff-826e-e93dd4c23994",
        "outputId": "a2bc256c-3232-499f-a960-502cf4390985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: torch.Size([1, 4]) type: torch.DoubleTensor\n",
            "y_train: torch.Size([1, 4]) type: torch.DoubleTensor\n",
            "X_train: tensor([[ 0.4006, -2.1999, -0.1462, -0.0383]], dtype=torch.float64)\n",
            "y_train: tensor([[-0.1462, -0.0383, -0.6669,  7.3712]], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test: torch.Size([1, 4]) type: torch.DoubleTensor\n",
            "y_test: torch.Size([1, 4]) type: torch.DoubleTensor\n",
            "X_test: tensor([[ 1.9537,  0.2797, -0.0780, -0.2268]], dtype=torch.float64)\n",
            "y_test: tensor([[-0.0780, -0.2268, -9.3138, -3.6618]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "for (X_train, y_train) in train_dataloader:\n",
        "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
        "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
        "    print('X_train:', X_train)\n",
        "    print('y_train:', y_train)\n",
        "    break\n",
        "\n",
        "for (X_test, y_test) in test_dataloader:\n",
        "    print('X_test:', X_test.size(), 'type:', X_test.type())\n",
        "    print('y_test:', y_test.size(), 'type:', y_test.type())\n",
        "    print('X_test:', X_test)\n",
        "    print('y_test:', y_test)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ee5f9916-4dc6-4a63-8329-c36811e77460",
      "metadata": {
        "id": "ee5f9916-4dc6-4a63-8329-c36811e77460"
      },
      "outputs": [],
      "source": [
        "class QCircuit:\n",
        "    \"\"\" \n",
        "    This class defines filter circuit of Quanvolution layer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_qubit, backend, shots):\n",
        "        # --- Circuit definition start ---\n",
        "        self.n_qubits = num_qubit\n",
        "        self._circuit = qiskit.QuantumCircuit(self.n_qubits,2)\n",
        "        \n",
        "        all_qubits = [i for i in range(self.n_qubits)]        \n",
        "        self._circuit.h(all_qubits)\n",
        "        \n",
        "        self.theta = [qiskit.circuit.Parameter('theta{}'.format(i)) for i in range(self.n_qubits)]\n",
        "\n",
        "        for i in range(self.n_qubits):\n",
        "            self._circuit.rx(self.theta[i], i)\n",
        "        \n",
        "        self._circuit.cx(0,2)\n",
        "        self._circuit.cx(1,3)\n",
        "        \n",
        "        self._circuit.barrier()\n",
        "        \n",
        "        self._circuit.measure(2,0)\n",
        "        self._circuit.measure(3,1)\n",
        "        \n",
        "        # ---- Circuit definition end ----\n",
        "\n",
        "        self.backend   = backend\n",
        "        self.shots     = shots\n",
        "\n",
        "    def run(self, data):\n",
        "        ListTarget = []\n",
        "        for valT in data.detach().numpy():\n",
        "            for j in range(len(valT)):\n",
        "                ListTarget.append(valT[j])\n",
        "        param_dict = dict()\n",
        "        \n",
        "        i= 0\n",
        "        for theta in ListTarget:\n",
        "            param_dict[self.theta[i]] = theta#[i]\n",
        "            i += 1\n",
        "        param_binds = [param_dict]\n",
        "        \n",
        "        t_qc = transpile(self._circuit,\n",
        "                         self.backend)\n",
        "        qobj = assemble(t_qc,\n",
        "                        shots=self.shots,\n",
        "                        parameter_binds = param_binds)\n",
        "        job = self.backend.run(qobj)\n",
        "        result = job.result().get_counts()\n",
        "        \n",
        "        probabilities = []\n",
        "        counts = 0\n",
        "        for key, val in result.items():\n",
        "            cnt = sum([int(char) for char in key])\n",
        "            counts += cnt * val\n",
        "\n",
        "            # Compute probabilities for each state\n",
        "            probab = counts / (self.shots * self.n_qubits)\n",
        "            probabilities.append(probab)\n",
        "        \n",
        "        return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f4d7ec78-fc04-4bd8-b64a-43842314e26b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "f4d7ec78-fc04-4bd8-b64a-43842314e26b",
        "outputId": "6a530727-63c8-49c9-f0ba-25f2d12f3714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0625, 0.1175, 0.2475]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">     ┌───┐┌────────────┐           ░       \n",
              "q_0: ┤ H ├┤ Rx(theta0) ├──■────────░───────\n",
              "     ├───┤├────────────┤  │        ░       \n",
              "q_1: ┤ H ├┤ Rx(theta1) ├──┼────■───░───────\n",
              "     ├───┤├────────────┤┌─┴─┐  │   ░ ┌─┐   \n",
              "q_2: ┤ H ├┤ Rx(theta2) ├┤ X ├──┼───░─┤M├───\n",
              "     ├───┤├────────────┤└───┘┌─┴─┐ ░ └╥┘┌─┐\n",
              "q_3: ┤ H ├┤ Rx(theta3) ├─────┤ X ├─░──╫─┤M├\n",
              "     └───┘└────────────┘     └───┘ ░  ║ └╥┘\n",
              "c: 2/═════════════════════════════════╩══╩═\n",
              "                                      0  1 </pre>"
            ],
            "text/plain": [
              "     ┌───┐┌────────────┐           ░       \n",
              "q_0: ┤ H ├┤ Rx(theta0) ├──■────────░───────\n",
              "     ├───┤├────────────┤  │        ░       \n",
              "q_1: ┤ H ├┤ Rx(theta1) ├──┼────■───░───────\n",
              "     ├───┤├────────────┤┌─┴─┐  │   ░ ┌─┐   \n",
              "q_2: ┤ H ├┤ Rx(theta2) ├┤ X ├──┼───░─┤M├───\n",
              "     ├───┤├────────────┤└───┘┌─┴─┐ ░ └╥┘┌─┐\n",
              "q_3: ┤ H ├┤ Rx(theta3) ├─────┤ X ├─░──╫─┤M├\n",
              "     └───┘└────────────┘     └───┘ ░  ║ └╥┘\n",
              "c: 2/═════════════════════════════════╩══╩═\n",
              "                                      0  1 "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#Testing the circuit\n",
        "backend = qiskit.Aer.get_backend('qasm_simulator')\n",
        "num_qubit = 4\n",
        "circ = QCircuit(num_qubit, backend, 100)\n",
        "data = torch.tensor([[-0.2397, -2.9656, -0.1623, -0.0680]])\n",
        "\n",
        "print(circ.run(data))\n",
        "circ._circuit.draw()#output='mpl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9913cd6c-42a7-44e9-adb7-d59c99b8d241",
      "metadata": {
        "id": "9913cd6c-42a7-44e9-adb7-d59c99b8d241"
      },
      "outputs": [],
      "source": [
        "class HybridFunction(Function):\n",
        "    \"\"\" Hybrid quantum - classical function definition \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def forward(ctx, input, quantum_circuit, shift):\n",
        "        \"\"\" Forward pass computation \"\"\"\n",
        "        ##print('hFor:', input)\n",
        "        #print('hFor1:', input[0])\n",
        "        #print('hFor2:', input[0].tolist())\n",
        "        ctx.shift = shift\n",
        "        ctx.quantum_circuit = quantum_circuit\n",
        "        #expectation_z = []\n",
        "        #for i in range(len(input)):\n",
        "        #    expectation_z.append(ctx.quantum_circuit.run(input[i].tolist()))\n",
        "        #print('expectation_z',expectation_z)\n",
        "        expectation_z = ctx.quantum_circuit.run(input[0].tolist())\n",
        "        result = torch.tensor([expectation_z])\n",
        "        #result = torch.tensor(expectation_z)\n",
        "        ctx.save_for_backward(input, result)\n",
        "\n",
        "        return result\n",
        "        \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\" Backward pass computation \"\"\"\n",
        "        print('a')\n",
        "        input, expectation_z = ctx.saved_tensors\n",
        "        input_list = np.array(input.tolist())\n",
        "        \n",
        "        shift_right = input_list + np.ones(input_list.shape) * ctx.shift\n",
        "        shift_left = input_list - np.ones(input_list.shape) * ctx.shift\n",
        "        \n",
        "        gradients = []\n",
        "        for i in range(len(input_list)):\n",
        "            expectation_right = ctx.quantum_circuit.run(shift_right[i])\n",
        "            expectation_left  = ctx.quantum_circuit.run(shift_left[i])\n",
        "            \n",
        "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
        "            gradients.append(gradient)\n",
        "        gradients = np.array([gradients]).T\n",
        "        return torch.tensor([gradients]).float() * grad_output.float(), None, None\n",
        "\n",
        "class Hybrid(nn.Module):\n",
        "    \"\"\" Hybrid quantum - classical layer definition \"\"\"\n",
        "    \n",
        "    def __init__(self, backend, shots, shift):\n",
        "        super(Hybrid, self).__init__()\n",
        "        self.quantum_circuit = QuantumCircuit(1, backend, shots)\n",
        "        self.shift = shift\n",
        "        \n",
        "    def forward(self, input):\n",
        "        #print('3',input)\n",
        "        return HybridFunction.apply(input, self.quantum_circuit, self.shift)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "948bdcdc-1deb-4751-b8f2-f56e15d6c54d",
      "metadata": {
        "id": "948bdcdc-1deb-4751-b8f2-f56e15d6c54d"
      },
      "outputs": [],
      "source": [
        "class LagrangianNeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, D_in, hidden_list, D_out):\n",
        "        \"\"\"\n",
        "        Neural Network used to approximate a paramaterized system lagrangian\n",
        "        \"\"\"\n",
        "        super(LagrangianNeuralNetwork, self).__init__()\n",
        "        self.model_layers = torch.nn.ModuleList()\n",
        "\n",
        "        # input layer\n",
        "        self.model_layers.append(torch.nn.Linear(D_in, hidden_list[0]))\n",
        "        self.model_layers.append(torch.nn.Softplus())\n",
        "        # add all hiden layers\n",
        "        for i in range(1, len(hidden_list)):\n",
        "            self.model_layers.append(torch.nn.Linear(hidden_list[i-1], hidden_list[i]))\n",
        "            self.model_layers.append(torch.nn.Softplus())\n",
        "        \n",
        "        # output layer\n",
        "        self.model_layers.append(torch.nn.Linear(hidden_list[-1], D_out))\n",
        "        self.model_layers.append(torch.nn.Softplus())\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        applies all of the model layers, and returns the single output value,\n",
        "        which in this case is the lagrangian of the system, representing the\n",
        "        total energy\n",
        "        \"\"\"\n",
        "        for layer in self.model_layers:\n",
        "            x = layer(x)        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f45d0c2a-d0fd-49b9-a691-135b285cd5da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f45d0c2a-d0fd-49b9-a691-135b285cd5da",
        "outputId": "9e645b5d-5cc1-4d75-e771-d60e685290c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Checking for CUDA Device... ---\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# determine device\n",
        "print(\"--- Checking for CUDA Device... ---\")\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "27ecd2c5-40f2-4f6a-830b-829db4b5b8d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27ecd2c5-40f2-4f6a-830b-829db4b5b8d3",
        "outputId": "c27a85af-3959-45bd-976d-9a8fdf70d602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Constructing Model... ---\n"
          ]
        }
      ],
      "source": [
        "# organize data\n",
        "input_size = train_inputs.shape[1]\n",
        "output_size = 1  # for all lagrangian systems, output should be just a scalar energy value\n",
        "\n",
        "# build model\n",
        "print(\"--- Constructing Model... ---\")\n",
        "D_in = input_size  # state size\n",
        "# hidden_list = [D_in, 256, 256, 256, 256, 256]\n",
        "hidden_list = [D_in, 32, 64, 128, 256, 512, 256, 128, 64, 32]\n",
        "D_out = output_size\n",
        "lnn_model = LagrangianNeuralNetwork(D_in, hidden_list, D_out)\n",
        "#summary(lnn_model, (1, 28, 28), device='cpu')\n",
        "# set up training parameters\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-5\n",
        "momentum = 0.9\n",
        "num_epochs = 20\n",
        "optimizer = torch.optim.Adam(lnn_model.parameters(),\n",
        "                             lr=learning_rate,\n",
        "                             weight_decay=weight_decay)\n",
        "\n",
        "if os.path.isfile(\"model_weights.pth\"):\n",
        "    print(\"Re-loading existing weights!\")\n",
        "    checkpoint = torch.load(\"model_weights.pth\")\n",
        "    lnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e0e2effd-2ba3-499f-8864-99e426633872",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0e2effd-2ba3-499f-8864-99e426633872",
        "outputId": "f7a6e616-9f7f-442a-e286-43af7d097aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Beginning Training! ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([1, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [5%]\tLoss: 0.9506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [10%]\tLoss: 0.0107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [15%]\tLoss: 0.0101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [20%]\tLoss: 0.0098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [25%]\tLoss: 0.0094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [30%]\tLoss: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [35%]\tLoss: 0.0105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [40%]\tLoss: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [45%]\tLoss: 0.0102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [50%]\tLoss: 0.0099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [55%]\tLoss: 0.0095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [60%]\tLoss: 0.0099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [65%]\tLoss: 0.0098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [70%]\tLoss: 0.0090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [75%]\tLoss: 0.0094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [80%]\tLoss: 0.0090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [85%]\tLoss: 0.0091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [90%]\tLoss: 0.0088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [95%]\tLoss: 0.0087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [100%]\tLoss: 0.0086\n",
            "end\n"
          ]
        }
      ],
      "source": [
        "# ensure model is in train mode so gradients are properly calculated\n",
        "lnn_model.train()\n",
        "# load device to either GPU or CPU depending on hardware\n",
        "lnn_model.to(device)\n",
        "\n",
        "# set up loss function\n",
        "loss_fcn = torch.nn.MSELoss()\n",
        "\n",
        "# set up GradScaler to improve run speed\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "print(\"--- Beginning Training! ---\")\n",
        "loss_list = []\n",
        "lnn_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        for p in lnn_model.parameters(): p.grad = None\n",
        "        \n",
        "        #Quantum layer for data encoding\n",
        "        backend = qiskit.Aer.get_backend('qasm_simulator')\n",
        "        num_qubit = 4\n",
        "        circ = QCircuit(num_qubit, backend, 100)\n",
        "        \n",
        "        #Encoding data and target\n",
        "        dataInput = torch.squeeze(torch.tensor(circ.run(data)))\n",
        "        targetData = torch.tensor(circ.run(target))\n",
        "        \n",
        "        with torch.cuda.amp.autocast():\n",
        "            target_pred = solve_euler_lagrange(lnn_model.forward, dataInput.float())            \n",
        "            loss = loss_fcn(target_pred.unsqueeze(0), targetData.float())\n",
        "            \n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "        \n",
        "        # Optimize the weights\n",
        "        scaler.step(optimizer)\n",
        "        \n",
        "        # update the scale for next iteration\n",
        "        scaler.update()\n",
        "        \n",
        "        total_loss.append(loss.item())\n",
        "        \n",
        "    loss_list.append(sum(total_loss)/len(total_loss))\n",
        "    print('Training [{:.0f}%]\\tLoss: {:.4f}'.format(100. * (epoch + 1) / num_epochs, loss_list[-1]))\n",
        "\n",
        "print('end')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9e195214-e278-4278-8a2f-971133ccf007",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "9e195214-e278-4278-8a2f-971133ccf007",
        "outputId": "62330869-b537-4f92-c348-34a8c404b2df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(loss_list)\n",
        "plt.title('Hybrid LQDNN Quantum-Classical Training (Four qubit)')\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "34d52598-f6eb-4292-9a02-e50b016d98a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34d52598-f6eb-4292-9a02-e50b016d98a3",
        "outputId": "52f4b1d2-46ba-4f6b-f04f-0c92bf8e6d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Beginning Test! ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./sample_data/dataloader.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  batch_x = torch.as_tensor(batch_x)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([1, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance on test data:\tLoss: 0.0085\tAccuracy: 0.5%\n",
            "end\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Beginning Test! ---\")\n",
        "lnn_model.eval()\n",
        "total_lossTestHybridQC = []\n",
        "correctTestQC = 0\n",
        "for batch_idx, (data, target) in enumerate(test_dataloader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    \n",
        "    dataTest = torch.squeeze(torch.tensor(circ.run(data)))\n",
        "    targetTest = torch.tensor(circ.run(target))    \n",
        "    \n",
        "    with torch.cuda.amp.autocast():\n",
        "        target_pred = solve_euler_lagrange(lnn_model.forward, dataTest.float())\n",
        "        correctTestQC += target_pred.eq(targetTest.view_as(target_pred)).sum().item()\n",
        "        testLoss = loss_fcn(target_pred.unsqueeze(0), targetTest.float())       \n",
        "    total_lossTestHybridQC.append(testLoss.item())\n",
        "    \n",
        "batch_size = 1\n",
        "print('Performance on test data:\\tLoss: {:.4f}\\tAccuracy: {:.1f}%'.format(\n",
        "    sum(total_lossTestHybridQC) / len(total_lossTestHybridQC),\n",
        "    correctTestQC / len(test_dataloader) * 100 / batch_size)\n",
        "    )\n",
        "\n",
        "print('end')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f85e0dca-203a-4981-8264-a27a4f370628",
      "metadata": {
        "id": "f85e0dca-203a-4981-8264-a27a4f370628"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Qiskit v0.32.1 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "MultipleQubit_QC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}